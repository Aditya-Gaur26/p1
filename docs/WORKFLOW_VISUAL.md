# ğŸ¯ Project Workflow - Visual Overview

## Complete Pipeline (6 Phases)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    YOUR RAW MATERIALS                                â”‚
â”‚  ğŸ“„ PDFs (962 + 675 pages)   ğŸ“Š PowerPoint Slides                   â”‚
â”‚  â””â”€ data/raw/pdfs/            â””â”€ data/raw/slides/                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 1: EXTRACTION  â±ï¸ ~30 min                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Command: python src/data_processing/extract_pdfs.py                â”‚
â”‚           python src/data_processing/extract_slides.py              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  What happens:                                                       â”‚
â”‚  â€¢ Reads PDFs page by page                                           â”‚
â”‚  â€¢ Extracts text from slides + speaker notes                         â”‚
â”‚  â€¢ Removes headers, footers, page numbers                            â”‚
â”‚  â€¢ Chunks content semantically                                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Output:                                                             â”‚
â”‚  âœ“ data/processed/books/all_pdfs_combined.json                      â”‚
â”‚  âœ“ data/processed/slides/all_slides_combined.json                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  âš ï¸  CHECK: Open JSON files â†’ Verify text is readable                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 2: DATASET CREATION  â±ï¸ ~15 min                               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Command: python src/data_processing/create_dataset.py              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  What happens:                                                       â”‚
â”‚  â€¢ Reads extracted content from Phase 1                              â”‚
â”‚  â€¢ For each chunk:                                                   â”‚
â”‚    â”œâ”€ Generates 3-5 questions (diverse types)                        â”‚
â”‚    â”œâ”€ Creates paraphrased versions                                   â”‚
â”‚    â”œâ”€ Adds reasoning chains to answers                               â”‚
â”‚    â””â”€ Formats as: Instruction â†’ Response                             â”‚
â”‚  â€¢ Applies data augmentation (2x)                                    â”‚
â”‚  â€¢ Splits 90% train / 10% validation                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Output:                                                             â”‚
â”‚  âœ“ data/processed/train.jsonl    (3600 examples)                    â”‚
â”‚  âœ“ data/processed/val.jsonl      (400 examples)                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  ğŸ” CRITICAL CHECKPOINT: Run diagnostic!                             â”‚
â”‚     python diagnose.py                                               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Quality Score Target: 70+ / 100                                     â”‚
â”‚  If < 70 â†’ FIX DATA BEFORE TRAINING!                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 3: TRAINING  â±ï¸ ~6-8 hours (RTX 3060)                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Command: python src/training/fine_tune.py                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  What happens:                                                       â”‚
â”‚  1. Loads Qwen2.5-7B-Instruct (base model)                          â”‚
â”‚     â€¢ Downloads from HuggingFace (~14GB)                             â”‚
â”‚     â€¢ Quantizes to 4-bit (~3.5GB in VRAM)                            â”‚
â”‚  2. Adds LoRA adapters                                               â”‚
â”‚     â€¢ r=32 â†’ ~64M trainable parameters                               â”‚
â”‚     â€¢ Targets: q_proj, k_proj, v_proj, o_proj, gates, embeddings    â”‚
â”‚  3. Trains for 3 epochs                                              â”‚
â”‚     â€¢ Batch size=1, grad accumulation=16 (effective batch=16)        â”‚
â”‚     â€¢ Learning rate=5e-5 with cosine schedule                        â”‚
â”‚     â€¢ Gradient checkpointing + 4-bit optimizer                       â”‚
â”‚  4. Saves checkpoints every 200 steps                                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Output:                                                             â”‚
â”‚  âœ“ models/fine_tuned/adapter_model.bin  (~500MB)                    â”‚
â”‚  âœ“ models/fine_tuned/adapter_config.json                            â”‚
â”‚  âœ“ models/fine_tuned/logs/ (TensorBoard)                            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  ğŸ“Š MONITOR TRAINING:                                                â”‚
â”‚  â€¢ GPU usage:     nvidia-smi -l 1                                    â”‚
â”‚  â€¢ Training logs: tensorboard --logdir models/fine_tuned/logs        â”‚
â”‚  â€¢ Watch for: eval_loss should decrease like train_loss             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  âš ï¸  STOP IF: eval_loss increases while train_loss decreases         â”‚
â”‚              (= overfitting â†’ model memorizing, not learning)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 4: VECTOR DATABASE  â±ï¸ ~20 min                                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Command: python src/data_processing/build_vectordb.py              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  What happens:                                                       â”‚
â”‚  â€¢ Re-reads all PDF/slide content                                    â”‚
â”‚  â€¢ Chunks into 512-token segments (overlap=50)                       â”‚
â”‚  â€¢ Embeds each chunk with sentence-transformers                      â”‚
â”‚  â€¢ Stores vectors in ChromaDB                                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Output:                                                             â”‚
â”‚  âœ“ data/vectordb/course_materials/ (ChromaDB)                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Purpose: Retrieval-Augmented Generation (RAG)                       â”‚
â”‚  â€¢ At inference: retrieves relevant context for each question        â”‚
â”‚  â€¢ Prevents hallucination by grounding answers in PDFs               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 5: INFERENCE  â±ï¸ ~2 sec per query                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Command: python src/inference/query_processor.py                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  User Query: "What is virtual memory?"                               â”‚
â”‚       â†“                                                              â”‚
â”‚  1. Query Expansion                                                  â”‚
â”‚     "What is virtual memory?" â†’ "virtual memory, paging, swap"       â”‚
â”‚       â†“                                                              â”‚
â”‚  2. Hybrid Retrieval (RAG)                                           â”‚
â”‚     â€¢ Dense: Semantic similarity (embeddings)                        â”‚
â”‚     â€¢ Sparse: Keyword matching (BM25)                                â”‚
â”‚     â€¢ Reranking: Cross-encoder scoring                               â”‚
â”‚     â†’ Top 5 most relevant chunks from vector DB                      â”‚
â”‚       â†“                                                              â”‚
â”‚  3. Prompt Construction                                              â”‚
â”‚     Context: <5 retrieved chunks>                                    â”‚
â”‚     Question: What is virtual memory?                                â”‚
â”‚     Answer based on context:                                         â”‚
â”‚       â†“                                                              â”‚
â”‚  4. Model Generation                                                 â”‚
â”‚     Fine-tuned Qwen2.5 generates answer                              â”‚
â”‚     â€¢ Uses LoRA adapters trained on your data                        â”‚
â”‚     â€¢ Constrained to context from vector DB                          â”‚
â”‚       â†“                                                              â”‚
â”‚  5. Post-processing                                                  â”‚
â”‚     â€¢ Removes repetitions                                            â”‚
â”‚     â€¢ Cites sources (PDF page numbers)                               â”‚
â”‚       â†“                                                              â”‚
â”‚  Output: "Virtual memory is a memory management technique..."        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 6: EVALUATION  â±ï¸ ~10 min                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Command: python src/evaluation/evaluate_model.py                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  What happens:                                                       â”‚
â”‚  â€¢ Loads test questions from data/evaluation/endsem_questions.json  â”‚
â”‚  â€¢ Generates answers with your fine-tuned model                      â”‚
â”‚  â€¢ Compares with reference answers                                   â”‚
â”‚  â€¢ Calculates metrics:                                               â”‚
â”‚    â”œâ”€ BLEU: Word-level overlap (0-100)                               â”‚
â”‚    â”œâ”€ ROUGE-L: Longest common subsequence                            â”‚
â”‚    â”œâ”€ BERTScore: Semantic similarity                                 â”‚
â”‚    â””â”€ Faithfulness: Grounded in context?                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Target Scores (Good Model):                                         â”‚
â”‚  â€¢ BLEU > 40                                                         â”‚
â”‚  â€¢ ROUGE-L > 0.6                                                     â”‚
â”‚  â€¢ BERTScore > 0.85                                                  â”‚
â”‚  â€¢ Faithfulness > 90%                                                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  If scores low:                                                      â”‚
â”‚  â†’ Check training data quality (python diagnose.py)                  â”‚
â”‚  â†’ Try different hyperparameters                                     â”‚
â”‚  â†’ Add more training examples                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš¨ Why Models Hallucinate

### Root Cause Analysis:

```
LOW QUALITY DATA (50% of cases)
  â”œâ”€ Short answers (< 50 chars)
  â”œâ”€ Generic questions ("What is this?")
  â”œâ”€ No specific details from PDFs
  â””â”€ Fix: Improve create_dataset.py

OVERFITTING (30% of cases)
  â”œâ”€ Too many epochs (>5)
  â”œâ”€ High learning rate (>1e-4)
  â”œâ”€ Low LoRA rank (<16)
  â””â”€ Fix: Lower LR, fewer epochs, increase r

NO REFUSAL EXAMPLES (15% of cases)
  â”œâ”€ Model never trained to say "I don't know"
  â”œâ”€ Generates plausible-sounding text when uncertain
  â””â”€ Fix: Add 10-20 out-of-scope Q&A with refusals

WEAK RETRIEVAL (5% of cases)
  â”œâ”€ Vector DB empty or wrong chunks
  â”œâ”€ Retrieves irrelevant context
  â””â”€ Fix: Rebuild vector DB, improve chunking
```

---

## ğŸ¯ Your Action Plan (Bulls-Eye Training)

### Current Status:
- âœ… Config optimized for RTX 3060 12GB
- âœ… Memory settings tuned (batch=1, seq=1024)
- âš ï¸  Training data quality: UNKNOWN

### Step-by-Step:

#### 1. **Diagnose Current State** (5 min)
```bash
python diagnose.py
```
This checks:
- Training data quality (score /100)
- Config settings
- Vector DB status

#### 2. **Fix Data Issues** (if score < 70)
Edit [src/data_processing/create_dataset.py](src/data_processing/create_dataset.py):
- Line 130: Add minimum length check
- Line 140: Use specific question templates
- Line 280: Add refusal examples

Then regenerate:
```bash
python src/data_processing/create_dataset.py
python diagnose.py  # Re-check
```

#### 3. **Train Model** (6-8 hours)
```bash
# Start training
python src/training/fine_tune.py

# In another terminal, monitor:
nvidia-smi -l 1
tensorboard --logdir models/fine_tuned/logs
```

Watch for:
- âœ… `train_loss`: 2.5 â†’ 1.8 â†’ 1.2 (smooth decrease)
- âœ… `eval_loss`: 2.3 â†’ 1.7 â†’ 1.3 (follows train_loss)
- âŒ If `eval_loss` increases â†’ STOP (overfitting)

#### 4. **Build Vector DB** (20 min)
```bash
python src/data_processing/build_vectordb.py
```

#### 5. **Test Model** (manual)
```bash
python src/inference/query_processor.py
```

Test cases:
1. Question from PDFs â†’ Should answer correctly
2. Detailed technical question â†’ Should cite sources
3. Out-of-scope question â†’ Should refuse

#### 6. **Evaluate** (10 min)
```bash
python src/evaluation/evaluate_model.py
```

Target: Faithfulness > 90%

---

## ğŸ“Š Quick Reference

### Files to Monitor:

| File | Purpose | Check |
|------|---------|-------|
| `data/processed/train.jsonl` | Training data | Quality score > 70 |
| `configs/training_config.yaml` | Hyperparameters | LR=5e-5, r=32, epochs=3 |
| `models/fine_tuned/logs/` | Training metrics | eval_loss decreases |
| `data/vectordb/course_materials/` | RAG database | Document count > 1000 |

### Key Commands:

```bash
# Full pipeline (if starting fresh)
python src/data_processing/extract_pdfs.py
python src/data_processing/extract_slides.py
python src/data_processing/create_dataset.py
python diagnose.py                              # CHECK DATA!
python src/training/fine_tune.py
python src/data_processing/build_vectordb.py
python src/inference/query_processor.py

# Just training (if data exists)
python diagnose.py                              # ALWAYS check first!
python src/training/fine_tune.py
```

### Troubleshooting:

| Problem | Solution |
|---------|----------|
| OOM error | Reduce `max_seq_length: 1024 â†’ 512` |
| Hallucinating | Add refusal examples, lower LR |
| Generic answers | Improve training data specificity |
| Slow training | Reduce to 2 epochs |
| Wrong answers | Check vector DB retrieval |

---

## ğŸ“š Documentation Guide

Read in this order:
1. [QUICKSTART.md](../QUICKSTART.md) - Basic setup & commands
2. **WORKFLOW_VISUAL.md** (this file) - Understanding the pipeline
3. [COMPLETE_WORKFLOW_AND_FIXES.md](COMPLETE_WORKFLOW_AND_FIXES.md) - Deep dive into hallucination fixes

---

**Next Step**: Run `python diagnose.py` to check your training data! ğŸ¯
