# Project Explanation

## ğŸ¯ Overview

This project creates an **intelligent tutoring system** for Operating Systems and Networks by fine-tuning a Qwen3 language model and enriching it with multiple advanced features.

## ğŸ—ï¸ Architecture Breakdown

### 1. **Data Processing Pipeline**

**Purpose**: Convert raw course materials into training data

**Components**:
- **`extract_slides.py`**: Extracts text from PowerPoint presentations using python-pptx
- **`extract_pdfs.py`**: Extracts text from PDF books using PyPDF2 and pdfplumber
- **`create_dataset.py`**: Generates instruction-response pairs for fine-tuning
- **`build_vectordb.py`**: Creates vector embeddings and stores in ChromaDB for RAG

**Flow**:
```
Raw Materials â†’ Extraction â†’ Cleaning â†’ Chunking â†’ Training Dataset
                                                  â†“
                                            Vector Database
```

### 2. **Fine-tuning System**

**Purpose**: Adapt Qwen3 model to course-specific knowledge

**Technology**:
- **LoRA (Low-Rank Adaptation)**: Parameter-efficient fine-tuning
  - Only trains ~1-2% of model parameters
  - Maintains base model knowledge
  - Reduces memory requirements

- **Quantization**: 8-bit quantization using bitsandbytes
  - Reduces model size by 4x
  - Enables training on consumer GPUs
  - Minimal quality loss

**Process**:
1. Load base Qwen3 model
2. Apply LoRA adapters to attention layers
3. Train on instruction-response pairs
4. Save adapter weights (not full model)

### 3. **RAG (Retrieval-Augmented Generation)**

**Purpose**: Ground model responses in course materials

**How it works**:
1. User asks question
2. Convert question to embedding (vector)
3. Search vector database for similar content
4. Retrieve top-k most relevant chunks
5. Inject into model prompt as context
6. Model generates answer using both learned knowledge and retrieved context

**Benefits**:
- Reduces hallucination
- Provides citations
- Allows updating knowledge without retraining
- Better factual accuracy

### 4. **Enrichment Features**

#### YouTube Suggester
**Purpose**: Find relevant educational videos

**Process**:
1. Extract key concepts from question
2. Search YouTube using Data API v3
3. Filter by quality metrics (views, ratings)
4. Rank by relevance to question
5. Return top videos with metadata

**Fallback**: Pre-curated list when API unavailable

#### Research Paper Search
**Purpose**: Recommend academic papers

**Process**:
1. Query arXiv API with question keywords
2. Filter by categories (cs.OS, cs.NI, etc.)
3. Calculate relevance scores
4. Return papers with abstracts and links

**Fallback**: Classic papers database

### 5. **Query Processing System**

**Purpose**: Orchestrate all components into unified pipeline

**Flow**:
```
Question
   â†“
RAG Retrieval (parallel)
   â†“
Context + Question â†’ Model â†’ Answer
   â†“                            â†“
Enrichment (parallel)    â†â”€â”€â”€â”€â”€â”€â”˜
   â”œâ”€ YouTube
   â””â”€ Papers
   â†“
Formatted Response
```

**Features**:
- Parallel processing where possible
- Graceful degradation (works without APIs)
- Configurable detail levels
- Source attribution

### 6. **Evaluation Framework**

**Purpose**: Measure system performance

**Metrics**:
- **ROUGE**: Text overlap with reference answers
- **BLEU**: Translation-quality metric
- **F1**: Precision/recall balance
- **Enrichment stats**: Video/paper coverage
- **Performance**: Response time

## ğŸ”¬ Technical Deep Dive

### LoRA Fine-tuning

**What is LoRA?**
- Adds small "adapter" matrices to model layers
- Instead of updating all weights W, we add Î”W = BA
- Where B and A are small low-rank matrices

**Why use LoRA?**
- **Memory efficient**: Train <1GB instead of >20GB
- **Fast**: Fewer parameters to update
- **Modular**: Can swap adapters for different courses

**Configuration**:
```yaml
lora:
  r: 16              # Rank of adapter matrices
  lora_alpha: 32     # Scaling factor
  target_modules:    # Which layers to adapt
    - q_proj         # Query projection
    - k_proj         # Key projection
    - v_proj         # Value projection
    - o_proj         # Output projection
```

### Vector Database (ChromaDB)

**Why ChromaDB?**
- **Persistent**: Saves to disk
- **Fast**: Optimized similarity search
- **Simple**: Easy to use API
- **Local**: No external dependencies

**Embedding Model**: 
- `sentence-transformers/all-MiniLM-L6-v2`
- 384-dimensional vectors
- Fast and accurate for semantic search

**Storage**:
```
Each document â†’ 384-dim vector
    â†“
Indexed in ChromaDB
    â†“
Query â†’ Find similar vectors
    â†“
Return original documents
```

### Training Strategy

**Instruction Tuning**:
- Format: Instruction-Response pairs
- Similar to Alpaca, Vicuna datasets
- Adapts model to Q&A format

**Data Augmentation**:
- Generate multiple questions per content chunk
- Vary question styles (definition, explanation, comparison)
- Extract topics for targeted questions

**Hyperparameters**:
```yaml
learning_rate: 2e-4      # Higher than pre-training
epochs: 3                # Avoid overfitting
batch_size: 4            # With gradient accumulation
warmup_ratio: 0.03       # Gradual learning rate increase
```

## ğŸ’¡ Design Decisions

### Why Qwen3?
1. **Performance**: State-of-the-art for its size
2. **Efficiency**: Smaller models (0.6B-32B) available
3. **License**: Apache 2.0 (permissive)
4. **Multilingual**: If needed later
5. **Context length**: Up to 32K tokens

### Why separate RAG and Fine-tuning?
- **Complementary**: RAG provides current context, fine-tuning provides understanding
- **Updatable**: Can update vector DB without retraining
- **Quality**: Reduces hallucination, improves citations

### Why YouTube + Papers?
- **Multimodal learning**: Different learning styles
- **Depth**: Papers for deep understanding
- **Breadth**: Videos for quick concepts
- **Real value**: Enhances beyond basic Q&A

## ğŸš€ Optimization Techniques

### Memory Optimization
1. **8-bit quantization**: Reduces model size
2. **Gradient checkpointing**: Trades compute for memory
3. **LoRA**: Trains only small adapters
4. **Batch accumulation**: Effective larger batches

### Speed Optimization
1. **Parallel enrichment**: YouTube and papers fetched together
2. **Vector DB indexing**: Fast similarity search
3. **Caching**: Results can be cached
4. **Batching**: Process multiple questions efficiently

### Quality Optimization
1. **RAG**: Grounds responses in course materials
2. **Fine-tuning**: Adapts to domain language
3. **Filtering**: Quality thresholds for videos/papers
4. **Validation**: Evaluation metrics guide improvement

## ğŸ“ Educational Value

### For Students:
- **Instant help**: 24/7 tutor availability
- **Multiple resources**: Answer + videos + papers
- **Citations**: Know where information comes from
- **Consistency**: Same quality explanations

### For Evaluation:
- **Metrics**: Quantitative performance measurement
- **Reproducible**: Clear evaluation protocol
- **Extensible**: Easy to add new features
- **Documented**: Comprehensive explanations

## ğŸ”® Future Enhancements

Potential additions:
1. **Web UI**: Gradio/Streamlit interface
2. **Diagram generation**: Visual explanations
3. **Code execution**: Run OS/network examples
4. **Flashcards**: Auto-generate study materials
5. **Quiz mode**: Test understanding
6. **Multi-course**: Extend to other subjects
7. **Conversation memory**: Multi-turn dialogues
8. **Voice interface**: Speech input/output

## ğŸ“Š Expected Results

**With proper course materials**:
- ROUGE-L: 0.6-0.8
- F1 Score: 0.7-0.85
- YouTube coverage: >90% questions
- Paper coverage: >80% questions
- Response time: 2-5 seconds

**Quality depends on**:
- Amount of training data
- Quality of course materials
- Model size selected
- Training duration
- API availability

## ğŸ¯ Grading Criteria Coverage

âœ… **Fine-tuning**: LoRA-based parameter-efficient training
âœ… **Course material**: Slides, books, notes processing
âœ… **Vector DB**: ChromaDB for RAG
âœ… **YouTube suggestions**: API integration with quality filtering
âœ… **Research papers**: arXiv search with relevance ranking
âœ… **Question solving**: Multi-format answers with citations
âœ… **Extensibility**: Modular architecture, easy to extend
âœ… **Evaluation**: Comprehensive metrics and reporting
âœ… **Documentation**: README, QUICKSTART, code comments
âœ… **GitHub ready**: .gitignore, requirements.txt, proper structure

## ğŸ“ Summary

This is a **production-grade** educational AI system that combines:
- Modern LLM fine-tuning techniques
- Retrieval-augmented generation
- Multi-source enrichment
- Comprehensive evaluation

All designed to create an **intelligent, reliable, and useful** tutoring assistant for Operating Systems and Networks.
