# Training Configuration for Fine-tuning Qwen3

# Model Configuration
model:
  name: "Qwen/Qwen2.5-7B-Instruct"  # Can change to 0.6B, 1.5B, 3B, 14B, or 32B
  cache_dir: "./models/base"
  trust_remote_code: true
  use_flash_attention_2: true    # Enable Flash Attention 2 for 2-4x speedup
  attn_implementation: "flash_attention_2"

# LoRA Configuration (Optimized)
lora:
  r: 32                          # Increased rank for better capacity (16→32)
  lora_alpha: 64                 # Keep alpha = 2*r for optimal scaling
  target_modules:                # Comprehensive module coverage
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj
    - embed_tokens              # NEW: Include embedding layer for better adaptation
  lora_dropout: 0.05             # Reduced dropout for better learning (0.1→0.05)
  bias: "none"
  task_type: "CAUSAL_LM"

# Quantization (More Aggressive)
quantization:
  load_in_8bit: false            # Using 4-bit instead for better efficiency
  load_in_4bit: true             # More aggressive quantization
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true  # Additional compression layer

# Training Parameters (Optimized for RTX 3060 12GB with Large Dataset)
training:
  output_dir: "./models/fine_tuned"
  num_train_epochs: 3            # 3 epochs for large dataset (5→3)
  per_device_train_batch_size: 1 # Minimum for 12GB VRAM (4→1)
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 16 # Compensate for small batch (8→16)
  gradient_checkpointing: true
  gradient_checkpointing_kwargs:
    use_reentrant: false         # Better memory efficiency
  learning_rate: 5.0e-5          # Lower LR for stability (2e-4→5e-5)
  weight_decay: 0.001
  warmup_ratio: 0.1              # Longer warmup (0.03→0.1)
  warmup_steps: 0                # Use ratio instead
  lr_scheduler_type: "cosine"    # Stable for long training
  
  # Optimization
  optim: "paged_adamw_8bit"      # Memory-efficient optimizer
  max_grad_norm: 1.0             # Less aggressive clipping (0.3→1.0)
  fp16: true                     # RTX 3060 supports FP16 well
  bf16: false                    # Disable BF16 for RTX 3060
  tf32: true                     # Enable TF32 for Ampere GPUs
  
  # Logging & Saving
  logging_steps: 25              # Less frequent for speed
  logging_first_step: true
  save_steps: 200                # Less frequent for large dataset (50→200)
  save_total_limit: 3            # Keep fewer checkpoints to save disk (5→3)
  evaluation_strategy: "steps"
  eval_steps: 200                # Match save_steps
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  
  # Early Stopping
  early_stopping_patience: 3     # Stop if no improvement for 3 evals
  early_stopping_threshold: 0.001
  
  # Data - MEMORY OPTIMIZED
  max_seq_length: 1024           # Reduced for memory (2048→1024)
  dataloader_num_workers: 2      # Parallel data loading
  dataloader_pin_memory: true    # Faster GPU transfer
  dataset_text_field: "text"
  packing: false
  
  # Advanced settings
  report_to: ["tensorboard"]     # Track with tensorboard
  remove_unused_columns: false
  ddp_find_unused_parameters: false

# Data Configuration
data:
  train_file: "./data/processed/train.jsonl"
  val_file: "./data/processed/val.jsonl"
  train_split: 0.9
  val_split: 0.1
  
# WandB Configuration (optional)
wandb:
  enabled: false
  project: "os-networks-finetuning"
  name: "qwen3-lora-finetune"
  
# Seed for reproducibility
seed: 42
